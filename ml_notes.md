## 数据挖掘需要做数据规范化
一般情况下是需要的，尤其是针对距离相关的运算，比如在 K-Means、KNN 以及聚类算法中，我们需要有对距离的定义，所以在做这些算法前，需要对数据进行规范化。

另外还有一些算法用到了梯度下降作为优化器，这是为了提高迭代收敛的效率，也就是提升找到目标函数最优解的效率。我们也需要进行数据规范化，比如逻辑回归、SVM 和神经网络算法。在这些算法中都有目标函数，需要对目标函数进行求解。梯度下降的目标是寻找到目标函数的最优解，而梯度的方法则指明了最优解的方向。

当然不是所有的算法都需要进行数据规范化。在构造决策树的时候，可以不用提前做数据规范化，因为我们不需要关心特征值的大小维度，也没有使用到梯度下降来做优化，所以数据规范化对决策树的构造结果和构造效率影响不大。除此之外，还是建议你在做数据挖掘算法前进行数据规范化。


### 决策树
构造过程需要决定三个问题：作为根结点、子节点的属性有哪些，何时停止得到最佳状态（叶节点）。

从训练集归纳出一组不相矛盾的分类规则，可能有多个，可能没有。

决策树的生成对应于模型的局部选择，剪枝对应于模型的全剧选择。决策树的生成只考虑局部最优，剪枝考虑全局最优。

属性过多容易过拟合，需要剪枝，剪枝分为预剪枝和后剪枝

预剪枝是在决策树构造时就进行剪枝。构造时对*节点*进行评估，如果节点在验证集中不能带来准确性的提升，就把当前节点作为叶节点，不对其进行划分。后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个*节点子树*，与保留该节点子树在分类准确性上差别不大，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

ID3 和 C4.5 算法可以生成二叉树或多叉树，而 CART 只支持二叉树。同时 CART 决策树比较特殊，既可以作分类树，又可以作回归树。如果我构造了一棵决策树，想要基于数据判断这个人的职业身份，这个就属于分类树，因为是从几个分类中来做选择。如果是给定了数据，想要预测这个人的年龄，那就属于回归树。分类树可以处理离散数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对连续型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。

决策树的核心就是寻找纯净的划分，因此引入了纯度的概念。在属性选择上，我们是通过统计“不纯度”来做判断的，ID3 是基于信息增益做判断，信息增益偏向于x取值较多的特征。C4.5 在 ID3 的基础上做了改进，提出了信息增益率的概念。实际上 CART 分类树与 C4.5 算法类似，只是属性选择的指标采用的是基尼系数。基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。在 CART 分类树中采用的是基尼系数作为标准，那么在 CART 回归树中，如何评价“不纯度”呢？实际上我们要根据样本的混乱程度，也就是样本的离散程度来评价“不纯度”。这两种节点划分的标准，分别对应着两种目标函数最优化的标准，即用最小绝对偏差（LAD），或者使用最小二乘偏差（LSD）。这两种方式都可以让我们找到节点划分的方法，通常使用最小二乘偏差的情况更常见一些。

CART 决策树的剪枝主要采用的是后剪枝 CCP 方法，cost-complexity prune，代价复杂度。这种剪枝方式用到一个指标叫做节点的表面误差率增益值，以此作为剪枝前后误差的定义。

### 朴素贝叶斯
假设：每个输入变量是独立的
概率：类别概率、每个类别属性的条件概率
贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。
朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。

#### 准备阶段
确定特征属性 --> 获取训练样本

#### 训练阶段
计算每个类别P(Cj) --> 计算每个类别在训练样本中出现频率、每个类别属性的条件概率，生成分类器

#### 应用阶段
使用分类器对测试数据分类

#### sklearn
- GaussianNB：特征变量是连续变量、符合高斯分布
- MultinomialNB：特征变量是离散变量、符合多项分布
- BernoulliNB：特征变量是不二变量、符合0-1分布

计算TF-IDF，较高的属性更适合做分类

当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。
当 0<alpha<1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。


### SVM
Support Vector Machine
模式识别、分类、回归分析，在文本分类尤其是针对二分类任务性能卓越
SVM帮助找到一个超平面用以分类
多维空间中，为保证决策面不变，且分类不产生错误的情况下，我们可以移动决策面 C，直到产生两个极限的位置：决策面 A 和决策面 B。极限的位置是指，如果越过了这个位置，就会产生分类错误。这样的话，两个极限位置 A 和 B 之间的分界线 C 就是最优决策面。极限位置到最优决策面 C 之间的距离，就是“分类间隔”，英文叫做 margin。
如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。

SVM 本身是一个二值分类器，最初是为二分类问题设计的，而实际上我们要解决的问题，可能是多分类的情况，比如对文本进行分类，或者对图像进行识别。针对这种情况，我们可以将多个二分类器组合起来形成一个多分类器，常见的方法有“一对多法”和“一对一法”两种。
- 一对多法
假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM。
针对 K 个分类，需要训练 K 个分类器，分类速度较快，但训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。
- 一对一法
可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。
当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。这样做的好处是，如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。但这种方法的不足在于，分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。

完全线性可分情况下的线性分类器，也就是线性可分的情况，是最原始的 SVM，它最核心的思想就是找到最大的分类间隔；
大部分线性可分情况下的线性分类器，引入了软间隔的概念。软间隔，就是允许一定量的样本分类错误；
线性不可分情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到了一个高维的空间中，从而变得线性可分。

4种核函数
- 线性核函数
数据线性可分情况下使用，运算速度快，效果好
不足：不能处理线性不可分数据
- 多项式核函数
可以将数据从低维空间映射到高维空间，但参数比较多，计算量大
- 高斯核函数（默认）
可以将数据从低维空间映射到高维空间，参数比多项式核函数需要的少，性能不错
- sigmoid核函数
多层神经网络

model = svm.SVC(kernel='rbf', C=1.0, gamma='auto')
C：目标函数的惩罚系数，分错样本时的惩罚程度，默认情况为1.0.当C越大的时候，分类器的准确性越高，容错率越低，泛化能力变差。
gamma：核函数的系数，默认为样本特征数的倒数

线性SVM分类器使用model=svm.LinearSVC()，没有kernel参数，对于数据大的线性可分问题，效果比SVC更高


### KNN
计算过程：
1. 计算待分类物体与其他物体之间的距离
2. 统计距离最近的k个邻居
3. 对于k个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类

k值选择很重要。太小容易过拟合（邻居是噪声，待分类物体分类也会产生误差）。太大可能欠拟合（距离过远的点也会对待分类物体的分类产生影响，没有真正分类）。因此采用交叉验证取k值：样本集中的大部分样本作为训练集，其余为验证集。一般把k值选取在较小范围内，同时在验证集上准确率最高的即为k值。

距离计算方式：欧式距离（最常用），曼哈顿距离，闵可夫斯基距离（p=1，曼哈顿；p=2，欧式；p接近无穷，切比雪夫），切比雪夫距离，余弦距离（方向上计算两者间差异，在兴趣相关性比较上角度关系比距离绝对值更重要，如搜索引擎关联推荐）

### K-Means
无监督，聚类
K类，Means代表中心
随机选择中心点、分类数，计算各个点到各中心点的距离，并归类于距离最短的那个中心点；再重新计算中心点、距离、归类，直到类别不再变化。

K值需要事先指定，如果不知道应该聚成几类，最好多设几组k值，选择聚类结果最好的那个。

### EM
聚类，事先知道聚类的个数，不知道每个样本分别属于哪一类，将潜在类别当作隐变量。
GMM高斯混合模型（样本符合高斯分布），每个高斯分布都属于这个模型的组成部分，要分成k类就相当于是k个组成部分，初始化每个组成部分的高斯分布的参数，再看每个样本属于哪个组成部分，这就是E步骤。再通过得到的这些隐变量结果，反过来求每个组成部分高斯分布的参数，即M步骤，反复EM，直到每个组成部分的高斯分布参数不变为止。
HMM隐马尔可夫模型


### PageRank
一个网页的影响力 = 所有入链集合的页面的加权影响力之和
实践中的问题：
1. 等级泄露（Rank Leak）：如果一个网页没有出链，就像是一个黑洞一样，吸收了其他网页的影响力而不释放，最终会导致其他网页的 PR 值为 0。
2. 等级沉没（Rank Sink）：如果一个网页只有出链，没有入链，计算的过程迭代下来，会导致这个网页的 PR 值为 0（也就是不存在公式中的 V）。
针对等级泄露的情况，可以把没有出链的节点先从图中去掉，等计算完所有节点的 PR 值之后，再加上该节点进行计算。不过这种方法会导致新的等级泄露的节点的产生，所以工作量还是很大的。

同时解决等级泄露和等级沉没这两个问题--PageRank 的随机浏览模型
用户并不都是按照跳转链接的方式来上网，还有一种可能是不论当前处于哪个页面，都有概率访问到其他任意的页面，比如说用户就是要直接输入网址访问其他页面，虽然这个概率比较小。定义一个阻尼因子 d，代表用户按照跳转链接来上网的概率，通常可以取一个固定值 0.85，而 1-d=0.15 则代表了用户不是通过跳转链接的方式来访问网页的，比如直接输入网址。


## regression
loss function 评价goodniss
通过gradient descent找到使loss function最小的模型参数，其中涉及到leaning rate（可用adagrad修正）
gradient descent 可能会陷入local minima、saddle point、 plateau
模型太复杂容易导致过拟合，因此大部分情况下可以使用regularization（假设smoother function is better）
模型效果不好可能是因为hidden factor，若有分类数据应可视化类别寻找规律。

### error的来源：bias + variance
假设m为N个样本点的平均值，mu为总体均值,sigma为总体标准差
除非N趋于无穷大，否则m不等于mu
但是当对m取期望值时，E[m] = mu，无偏估计
var[m] = sigma^2/N  N很大时，m围绕mu越密集，N很小时，越分散
bias是对所有的估计量求期望，是否接近总体值，bias 越小，越接近

s^2 = sum(x^n-m)^2 / N  
E(s^2) = (N-1)*sigma^2 / N  比sigma^2小，有偏估计，N越大，越集中在sigma^2附近

模型越复杂，variance越大（模型越简单，越不容易受样本数据影响，比如f(x)=c），bias越小
bias很大，会导致underfitting，variance很大导致overfitting 

如果模型不能很好的拟合训练数据（underfitting）则有较大bias；如果测试数据的error较大（overfitting），则有较大的variance

bias大时：redesign the model（增加样本量也无济于事），1. 增加更多特征；2. 设计更复杂的模型
variance大时：1. 增加样本量； 2.regularization（更平滑）

模型选择时（一般选择测试数据中error最小的模型）
由于测试数据是固定的，对于外界真实环境中更多更丰富的测试数据而言，也具有bias